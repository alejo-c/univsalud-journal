<p>La Revista Universidad y Salud se adhiere a la <strong>Declaración de San Francisco Sobre la Evaluación de la Investigación</strong>. <a href="https://sfdora.org/signers/?_signers_keyword=Revista%20Universidad%20y%20Salud&amp;_organization_country=colombia&amp;_signer_type=organisation">Firmantes</a></p>
<p>Existe una necesidad apremiante de mejorar la forma en que las agencias de financiación, las instituciones académicas y otros grupos evalúan la investigación científica. Para abordar este tema, un grupo de editores de revistas académicas se reunió durante la Reunión anual de la <em>American Society for Cell Biology</em> (ASCB) en San Francisco, California, el 16 de diciembre de 2012. Este grupo desarrolló una serie de recomendaciones, conocidas como la Declaración de San Francisco sobre la Evaluación de la Investigación. Invitamos a los grupos interesados de todas las disciplinas científicas a mostrar su apoyo añadiendo sus nombres a esta declaración.</p>
<p>Los productos de la investigación científica son muchos y variados, e incluyen: artículos de investigación que informan sobre nuevos conocimientos, datos, reactivos y software; propiedad intelectual y jóvenes científicos capacitados. Las agencias financiadoras, las instituciones que emplean científicos y los propios científicos, tienen el deseo y la necesidad de evaluar la calidad y el impacto de los resultados científicos. Por lo tanto, es imperativo que la producción científica se mida con precisión y se evalúe con prudencia.</p>
<p>El factor de impacto se utiliza con frecuencia como parámetro principal con el que comparar la producción científica de individuos e instituciones. El factor de impacto, calculado por Thomson Reuters*, se creó originalmente como una herramienta para ayudar a los bibliotecarios a identificar revistas para comprar, no como una medida de la calidad científica de la investigación en un artículo. Teniendo esto en cuenta, es fundamental comprender que el factor de impacto tiene una serie de deficiencias bien documentadas como herramienta para la evaluación de la investigación.</p>
<p>Estas limitaciones incluyen:</p>
<ol type="A">
<li class="show">Las distribuciones de citas dentro de las revistas son muy sesgadas [1-3],</li>
<li class="show">las propiedades del factor de impacto son específicas de cada campo: es un compuesto de múltiples tipos de artículos altamente diversos, incluyendo trabajos de investigación primaria y revisiones [1, 4],</li>
<li class="show">los factores de impacto pueden ser manipulados (o evaluados) por la política editorial [5], y</li>
<li class="show">los datos utilizados para calcular el factor de impacto no son transparentes ni están abiertamente disponibles para el público [4, 6, 7].</li>
</ol>
<p>A continuación, hacemos una serie de recomendaciones para mejorar la forma en que se evalúa la calidad de la producción científica. Los productos que no sean artículos de investigación crecerán en importancia a la hora de evaluar la eficacia de la investigación en el futuro, pero el documento de investigación revisado por pares seguirá siendo primordial para la evaluación de la investigación. Por lo tanto, nuestras recomendaciones se centran en las prácticas relacionadas con los artículos de investigación publicados en revistas revisadas por pares, pero pueden y deben ampliarse reconociendo productos adicionales, como los conjuntos de datos, ya que son productos de investigación importantes. Estas recomendaciones están dirigidas a agencias financiadoras, instituciones académicas, revistas, organizaciones que proporcionan métricas e investigadores individuales.</p>
<p>Estas recomendaciones cubren una serie de temas:</p>
<ul>
<li class="show">La necesidad de eliminar el uso de métricas basadas en revistas, tales como el factor de impacto, en consideraciones de financiamiento, nombramiento y promoción,</li>
<li class="show">la necesidad de evaluar la investigación por sus propios méritos en lugar de basarse en la revista en la que se publica la investigación, y</li>
<li class="show">la necesidad de capitalizar las oportunidades que ofrece la publicación en línea (como flexibilizar los límites innecesarios en el número de palabras, figuras y referencias en los artículos, y explorar nuevos indicadores de importancia e impacto).</li>
</ul>
<p>Reconocemos que múltiples agencias financiadoras, instituciones, editores e investigadores ya están fomentando mejores prácticas en la evaluación de la investigación. Dichos pasos están comenzando a aumentar el impulso hacia enfoques más sofisticados y significativos para la evaluación de la investigación que ahora pueden ser desarrollados y adoptados por todas las partes clave involucradas.</p>
<p>Los signatarios de la Declaración de San Francisco sobre la Evaluación de la Investigación apoyan la adopción de las siguientes prácticas en la evaluación de la investigación.</p>
<p><strong>Recomendación general</strong></p>
<ol>
<li class="show">No utilice métricas basadas en revistas, como el factor de impacto, como una medida sustituta de la calidad de los artículos de investigación individuales, para evaluar las contribuciones de un científico individual, o en las decisiones de contratación, promoción o financiación.</li>
</ol>
<p><strong>Para las agencias de financiación</strong></p>
<ol start="2">
<li class="show">Sea explícito sobre los criterios utilizados para evaluar la productividad científica de los solicitantes de fondos de investigación, especialmente para los investigadores que están iniciando su carrera investigadora, que el contenido científico de un artículo es mucho más importante que las métricas de publicación o la identidad de la revista en la que fue publicado.</li>
<li class="show">Con el fin de evaluar la investigación, considere el valor y el impacto de todos los resultados de la investigación (incluidos los conjuntos de datos y el software) además de las publicaciones de investigación, y considere una amplia gama de medidas de impacto que incluyan indicadores cualitativos, como la influencia sobre la política y prácticas científicas.</li>
</ol>
<p><strong>Para las instituciones</strong></p>
<ol start="4">
<li class="show">Sea explícito sobre los criterios utilizados para realizar decisiones de contratación, permanencia y promoción, destacando, especialmente para los investigadores que están iniciando su carrera investigadora, que el contenido científico de un trabajo es mucho más importante que las métricas de publicación o la identidad de la revista en la que fue publicado.</li>
<li class="show">Con el fin de evaluar la investigación, considere el valor y el impacto de todos resultados de la investigación (incluidos los conjuntos de datos y el software) además de las publicaciones de investigación, y considere una amplia gama de medidas de impacto, incluidos los indicadores cualitativos del impacto de la investigación, como la influencia sobre la política y prácticas científicas.</li>
</ol>
<p><strong>Para las editoriales</strong></p>
<ol start="6">
<li class="show">Reduzca profundamente el énfasis en el factor de impacto como herramienta promocional, idealmente dejando de promover su uso o presentando la métrica en el contexto de una variedad de métricas basadas en revistas (por ejemplo, factor de impacto de 5 años, EigenFactor [8], SCImago [9], h-index, tiempo editorial y de publicación, etc.) que proporcionan una visión más amplia del rendimiento de la revista.</li>
<li class="show">Ponga a disposición una variedad de métricas a nivel de artículo para alentar un cambio hacia la evaluación basada en el contenido científico de un artículo en lugar de las métricas de publicación de la revista en la que se publicó.</li>
<li class="show">Fomente las prácticas de la autoría responsable y la provisión de información sobre las contribuciones específicas de cada autor.</li>
<li class="show">Independientemente de que una revista sea de acceso abierto o basada en suscripciones, elimine todas las limitaciones de reutilización de las listas de referencias en los artículos de investigación y haga que estén disponibles bajo la dedicación de dominio público de Creative Commons [10].</li>
<li class="show">Elimine o reduzca las restricciones sobre el número de referencias en los artículos de investigación y, cuando corresponda, ordene la citación de la literatura primaria a favor de las revisiones para dar crédito al grupo o los grupos que primero informaron de un hallazgo.</li>
</ol>
<p><strong>Para las organizaciones que proporcionan métricas</strong></p>
<ol start="11">
<li class="show">Sea abierto y transparente al proporcionar datos y métodos utilizados para calcular las métricas.</li>
<li class="show">Proporcione los datos bajo una licencia que permita la reutilización sin restricciones y proporcione acceso computacional a los datos, cuando sea posible.</li>
<li class="show">Especifique que no se tolerará la manipulación inapropiada de las métricas; sea explícito sobre lo que constituye una manipulación inapropiada y qué medidas se tomarán para combatirla.</li>
<li class="show">Tenga en cuenta la variación en los tipos de artículos (por ejemplo, revisiones frente a artículos de investigación) y en las diferentes áreas temáticas al utilizar, agregar o comparar métricas.</li>
</ol>
<p><strong>Para los investigadores</strong></p>
<ol start="15">
<li class="show">Cuando participe en comités que toman decisiones sobre financiación, contratación, permanencia o promoción, realice evaluaciones basadas en el contenido científico en lugar de en métricas de publicación.</li>
<li class="show">Cuando sea apropiado, cite literatura primaria en que las observaciones son referidas primero, en lugar de revisiones para dar crédito donde debe darse.</li>
<li class="show">Utilice una gama de métricas e indicadores basadas en declaraciones personales y de apoyo, como evidencia del impacto de artículos individuales publicados y otros resultados de investigación [11].</li>
<li class="show">Impugne las prácticas de evaluación que dependen indebidamente del factor de impacto y promueva y enseñe prácticas que se centren en el valor y la influencia de los resultados de investigación específicos.</li>
</ol>
<p><strong>Referencias</strong></p>
<ol>
<li class="show"><span lang="en-US"> <a href="https://www.mathunion.org/fileadmin/IMU/Report/CitationStatistics.pdf" target="_blank" rel="noopener"> Adler R, Ewing J, Taylor P. Citation statistics. A report from the International Mathematical Union. 2008. </a> </span></li>
<li class="show"><span lang="en-US"> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2126010/pdf/9056804.pdf" target="_blank" rel="noopener"> Seglen PO. Why the impact factor of journals should not be used for evaluating research. BMJ. 1997; 314:498–502. </a> </span></li>
<li class="show"><span lang="en-US"> <a href="https://www.nature.com/articles/4351003b" target="_blank" rel="noopener"> Editorial. Not so deep impact. Nature. 2005; 435:1003–1004. </a> </span></li>
<li class="show"><span lang="en-US"> <a href="https://link.springer.com/article/10.1007%2Fs11192-011-0561-0" target="_blank" rel="noopener"> Vanclay JK. Impact Factor: Outdated artefact or stepping-stone to journal certification. Scientometric. 2012; 92:211–238. </a> </span></li>
<li class="show"><a href="http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0030291" target="_blank" rel="noopener"> <span lang="en-US"> The PLoS Medicine Editors. The impact factor game. PLoS Med. 2006; 3(6):e291. DOI: 10.1371/journal.pmed.0030291. </span> </a></li>
<li class="show"><a href="http://jcb.rupress.org/content/179/6/1091" target="_blank" rel="noopener"><span lang="en-US"> Rossner M, Van Epps H, Hill E. Show me the data. J Cell Biol. 2007; 179:1091–1092. </span></a></li>
<li class="show"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2213574/" target="_blank" rel="noopener"> <span lang="en-US"> Rossner M, Van Epps H, Hill E. Irreproducible results: A response to Thomson Scientific. J Cell Biol. 2008; 180:254–255. </span></a></li>
<li class="show"><a href="http://www.eigenfactor.org/" target="_blank" rel="noopener"> http://www.eigenfactor.org </a></li>
<li class="show"><a href="http://www.scimagojr.com/" target="_blank" rel="noopener"> http://www.scimagojr.com </a></li>
<li class="show"><a href="http://opencitations.wordpress.com/2013/01/03/open-letter-to-publishers" target="_blank" rel="noopener"> http://opencitations.wordpress.com/2013/01/03/open-letter-to-publishers </a></li>
<li class="show"><a href="http://altmetrics.org/tools/" target="_blank" rel="noopener"> http://altmetrics.org/tools </a></li>
</ol>
<p>* El <em>Journal Impact Factor </em>actualmente es publicado por Clarivate Analytics</p>
<div class="builder-text-row">
<div id="builder-section-text_39-column-1" class="builder-text-column builder-text-column-2">
<div class="builder-text-content"><hr class="ttfmake-hr" />
<p>PDF: <a href="https://sfdora.org/wp-content/uploads/2018/09/DORA_Spanish.pdf" target="_blank" rel="noopener">DORA_Spanish.pdf</a></p>
</div>
</div>
</div>
<div class="builder-text-row">
<div id="builder-section-text_39-column-1" class="builder-text-column builder-text-column-3">
<div class="builder-text-content"><hr class="ttfmake-hr" />
<p>This is a translation of the DORA text at <a href="https://sfdora.org/read" target="_blank" rel="noopener">https://sfdora.org/read</a>, contributed by Beatriz Pardal-Peláez. It is also published in Revista ORL (<a href="http://dx.doi.org/10.14201/orl.17845" target="_blank" rel="noopener">http://dx.doi.org/10.14201/orl.17845</a>) and is made available under the terms of the <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener">Creative Commons Attribution International License</a>. We are very grateful to the volunteers who have produced and checked the translations of the declaration. Errors might occasionally occur and if you do spot one, please contact <a href="mailto:info@sfdora.org">info@sfdora.org</a>.</p>
</div>
</div>
</div>
